{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "import pickle as pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "stopWords = stopwords.words('english')\n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(words):\n",
    "    return [word for word in words if word not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWords(words):\n",
    "    return [ps.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    try:\n",
    "        file=open(os.path.join(os.path.join(os.path.abspath('../binaries'),\"data\"),filename),\"r\", encoding=\"utf-8\")\n",
    "        tokens = [word.lower() for word in word_tokenize(file.read())]\n",
    "        tokens=[x for x in tokens if not any(c.isdigit() for c in x)]\n",
    "        tokens=removeStopWords(tokens)\n",
    "        tokens=stemWords(tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        raise Exception(\"../binaries/data/\"+filename+\" does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "        data.append(index)\n",
    "    return data, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(vocabulary_size=25000):\n",
    "    vocabulary = read_data(\"wikisent2.txt\")\n",
    "    data,dictionary = build_dataset(vocabulary,vocabulary_size)\n",
    "    del vocabulary\n",
    "    return data, dictionary "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "data, dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"window_size\",\"vocab_size\",\"dictionary\",\"labels\",\"word_target\",\"word_context\"]\n",
    "for name in names:\n",
    "    with open(os.path.join(os.path.join(os.path.abspath('../binaries'),\"pickles\"),name+\".pickle\"), \"wb\") as f:\n",
    "        pickle.dump(globals()[name], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
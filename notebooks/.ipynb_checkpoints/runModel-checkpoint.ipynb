{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1101 18:16:43.292366 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1101 18:16:43.307325 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1101 18:16:43.359158 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1101 18:16:43.360155 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1101 18:16:43.360155 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1101 18:17:21.825363 75052 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1101 18:17:21.833342 75052 deprecation.py:323] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(os.path.join(os.path.join(os.path.abspath('../binaries'),\"models\"),\"classification_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       7500000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 300, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 1)         0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,500,002\n",
      "Trainable params: 7,500,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"window_size\",\"vocab_size\",\"dictionary\"]\n",
    "for name in names:\n",
    "    with open(os.path.join(os.path.join(os.path.abspath('../binaries'),\"pickles\"),name+\".pickle\"), \"rb\") as f:\n",
    "        globals()[name]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(rankedWords):\n",
    "    return [word for word in rankedWords if word[0] not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNumbers(rankedWords):\n",
    "    return  [word for word in rankedWords if not any(char.isdigit() for char in word[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateInverseCounts(wordSequence):\n",
    "    countDict=dict()\n",
    "    for word in wordSequence:\n",
    "        if word not in countDict:\n",
    "            countDict[word]=1\n",
    "        else:\n",
    "            countDict[word]+=1\n",
    "    return [1/countDict[word] for word in wordSequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWordScores(wordSequence,tokenized):\n",
    "    wordScores=np.zeros(len(wordSequence))\n",
    "    contextCounts=np.zeros(len(wordSequence))\n",
    "    inverseCounts=generateInverseCounts(wordSequence)\n",
    "    for i in range(len(wordSequence)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(wordSequence)-1,i+window_size)\n",
    "        contextCounts[i]=rightBound-leftBound+1\n",
    "        for k in range(i+1,rightBound+1):\n",
    "            confidence=model.predict([[wordSequence[i]],[wordSequence[k]]])\n",
    "            wordScores[i]+=confidence[0][0]\n",
    "            wordScores[k]+=confidence[0][0]\n",
    "    return ([tokenized[i],inverseCounts[i]*wordScores[i]/contextCounts[i]] for i in range(len(wordSequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(string):\n",
    "    string=re.sub(r\"[^\\w\\s]\", ' ', string)\n",
    "    string=string.lower()\n",
    "    tokenized=string.split()\n",
    "    wordSequence=[]\n",
    "    for word in tokenized:\n",
    "        if word in dictionary:\n",
    "            wordSequence.append(dictionary[word])\n",
    "        else:\n",
    "            wordSequence.append(dictionary[\"UNK\"])\n",
    "    return wordSequence,tokenized,string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRankedList(string,**attributes):\n",
    "    string=\" \".join(string.split())\n",
    "    noNumbers=attributes[\"noNumbers\"] if \"noNumbers\" in attributes else True\n",
    "    noStopWords=attributes[\"noStopWords\"] if \"noStopWords\" in attributes else True\n",
    "    wordSequence,tokenized,string=formatData(string)\n",
    "    wordScores=findWordScores(wordSequence,tokenized)\n",
    "    wordBank={}\n",
    "    for word in wordScores:\n",
    "        if word[0] in wordBank:\n",
    "            wordBank[word[0]]=min(wordBank[word[0]],word[1])\n",
    "        else:\n",
    "            wordBank[word[0]]=word[1]\n",
    "    sortedList=sorted(wordBank.items(), key=lambda x: x[1])\n",
    "    rankedWords=[[x,y] for x,y in sortedList]\n",
    "    if noStopWords:\n",
    "        rankedWords=removeStopWords(rankedWords)\n",
    "    if noNumbers:\n",
    "        rankedWords=removeNumbers(rankedWords)\n",
    "    return rankedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printList(testCase,num=None,**attributes):\n",
    "    noNumbers=attributes[\"noNumbers\"] if \"noNumbers\" in attributes else True\n",
    "    noStopWords=attributes[\"noStopWords\"] if \"noStopWords\" in attributes else True\n",
    "    rankedWords=generateRankedList(testCase,noNumbers=noNumbers,noStopWords=noStopWords)\n",
    "    print(testCase)\n",
    "    if(num==None):\n",
    "        num=len(rankedWords)\n",
    "    else:\n",
    "        num=min(len(rankedWords),num)\n",
    "    print()\n",
    "    for i in range(num):\n",
    "        print(str(i+1)+\". \"+rankedWords[i][0]+\": \"+str(round(rankedWords[i][1],3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testCases=[\"\"\"\n",
    "# Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers in a low-dimensional space relative to the vocabulary size (\"continuous space\").\n",
    "# \"\"\",\"\"\"\n",
    "# Keras is an open-source neural-network library written in Python.\n",
    "# \"\"\",\"\"\"\n",
    "# Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.\n",
    "# \"\"\",\n",
    "# \"\"\"\n",
    "# Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
    "# \"\"\"\n",
    "# ]\n",
    "# for testCase in testCases:\n",
    "#     printList(testCase,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

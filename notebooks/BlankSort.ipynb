{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext as ft\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n  \"C extension not loaded, training will be slow. \"\n"
    }
   ],
   "source": [
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "model=ft.load_facebook_vectors(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3\n",
    "stemmer = nltk.wordnet.WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(text,top=0,collapse=True):\n",
    "    text=text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model.wv.vocab):\n",
    "            filtered_words.append(word)\n",
    "            counts.setdefault(word,0)\n",
    "            counts[word]+=1\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=model.wv.similarity(filtered_words[i],filtered_words[j])\n",
    "            similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        phraseScores=collapsePhrases(tokens,scores,filteredWordScores[:len(filteredWordScores)//3],cleaned_words)\n",
    "        if(top==0):\n",
    "            top=len(phraseScores)\n",
    "        return phraseScores[:top]\n",
    "    else:\n",
    "        if(top==0):\n",
    "            top=len(filteredWordScores)\n",
    "        return filteredWordScores[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePhrases(tokens,scores,filteredWordScores,cleaned_words):\n",
    "    dictionary=set()\n",
    "    for word in cleaned_words:\n",
    "        dictionary.add(word)\n",
    "    phrases=dict()\n",
    "    bagOfWords=set()\n",
    "    for word in filteredWordScores:\n",
    "        bagOfWords.add(word[0])\n",
    "    phrase=\"\"\n",
    "    totalScore=0\n",
    "    wordCount=0\n",
    "    i=0\n",
    "    while i in range(len(tokens)):\n",
    "        word=stemmer.lemmatize(tokens[i])\n",
    "        if (word in stops or tokens[i] in stops) and phrase!=\"\":\n",
    "            j=i+1\n",
    "            while j<len(tokens) and stemmer.lemmatize(tokens[j]) not in dictionary:\n",
    "                j+=1\n",
    "            if(j<len(tokens) and stemmer.lemmatize(tokens[j]) in bagOfWords):\n",
    "                for k in range(i,j):\n",
    "                    phrase+=tokens[k]+\" \"\n",
    "                i=j-1\n",
    "            else:\n",
    "                if(wordCount>0):\n",
    "                    phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                    totalScore=0\n",
    "                    wordCount=0\n",
    "                    phrase=\"\"\n",
    "        elif word in bagOfWords:\n",
    "            totalScore+=scores[word]\n",
    "            wordCount+=1\n",
    "            phrase+=tokens[i]+\" \"\n",
    "        else:\n",
    "            if(wordCount>0):\n",
    "                phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                totalScore=0\n",
    "                wordCount=0\n",
    "                phrase=\"\"\n",
    "        i+=1\n",
    "    phraseScores=list(map(list, phrases.items()))\n",
    "    phraseScores=sorted(phraseScores, key = lambda x: x[1])#,reverse=True)\n",
    "    return phraseScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  # This is added back by InteractiveShellApp.init_path()\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
    },
    {
     "data": {
      "text/plain": "[['word', 0.1458354284365972],\n ['learning', 0.16090598447933896],\n ['fasttext', 0.19167518741596723],\n ['facebook', 0.23167448633882617],\n ['model', 0.24257811784212077],\n ['library', 0.4467983305454254],\n ['embedding', 0.45143986865878105],\n ['created', 0.4735349738704307],\n ['network', 0.47493385321771103],\n ['available', 0.48399459198117256],\n ['classification', 0.48418148620320217],\n ['pretrained', 0.4844980499308024],\n ['language', 0.4857285368655409],\n ['make', 0.4907319114676544],\n ['research', 0.4967011834627816],\n ['neural', 0.4974703682320459],\n ['obtaining', 0.49845168300505194],\n ['lab', 0.5003112197986671],\n ['vector', 0.509221863001585],\n ['text', 0.5125238767692021],\n ['representation', 0.5140907155748989],\n ['create', 0.5158568439739091],\n ['allows', 0.5190341147993293],\n ['algorithm', 0.5199467938925538],\n ['embeddings', 0.523207523460899],\n ['supervised', 0.5408256676580224],\n ['unsupervised', 0.5429242226694312]]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(\"\"\"\n",
    "fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. fastText uses a neural network for word embedding.\n",
    "\"\"\",0,False)"
   ]
  }
 ]
}
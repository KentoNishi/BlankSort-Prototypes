{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, dot\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"vocab_size\",\"labels\",\"word_target\",\"word_context\",\"window_size\"]\n",
    "for name in names:\n",
    "    with open(os.path.join(os.path.join(os.path.abspath('../binaries'),\"pickles\"),name+\".pickle\"), \"rb\") as f:\n",
    "        globals()[name]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim=300\n",
    "epochs=200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 18:57:48.717042 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1014 18:57:48.744995 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 18:57:48.845725 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([target,context],axes=1,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  import sys\n",
      "W1014 18:57:48.988342 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1014 18:57:48.999313 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1014 18:57:49.003303 18820 deprecation.py:323] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 18:57:49.174255 18820 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6949827671051025\n",
      "Iteration 1000, loss=0.6922613978385925\n",
      "Iteration 2000, loss=0.687083899974823\n",
      "Iteration 3000, loss=0.6899444460868835\n",
      "Iteration 4000, loss=0.6994915008544922\n",
      "Iteration 5000, loss=0.7002225518226624\n",
      "Iteration 6000, loss=0.6933541893959045\n",
      "Iteration 7000, loss=0.7081913948059082\n",
      "Iteration 8000, loss=0.7037034034729004\n",
      "Iteration 9000, loss=0.6865083575248718\n",
      "Iteration 10000, loss=0.692814290523529\n",
      "Iteration 11000, loss=0.6690691709518433\n",
      "Iteration 12000, loss=0.6581525802612305\n",
      "Iteration 13000, loss=0.6827358603477478\n",
      "Iteration 14000, loss=0.6958131194114685\n",
      "Iteration 15000, loss=0.7059758901596069\n",
      "Iteration 16000, loss=0.6846375465393066\n",
      "Iteration 17000, loss=0.6785730123519897\n",
      "Iteration 18000, loss=0.6893879771232605\n",
      "Iteration 19000, loss=0.7080847024917603\n",
      "Iteration 20000, loss=0.6864486336708069\n",
      "Iteration 21000, loss=0.6967442035675049\n",
      "Iteration 22000, loss=0.726808488368988\n",
      "Iteration 23000, loss=0.6900349259376526\n",
      "Iteration 24000, loss=0.6790818572044373\n",
      "Iteration 25000, loss=0.7068383097648621\n",
      "Iteration 26000, loss=0.6861627101898193\n",
      "Iteration 27000, loss=0.6833056211471558\n",
      "Iteration 28000, loss=0.6944230794906616\n",
      "Iteration 29000, loss=0.6821077466011047\n",
      "Iteration 30000, loss=0.6940340995788574\n",
      "Iteration 31000, loss=0.688297688961029\n",
      "Iteration 32000, loss=0.693436861038208\n",
      "Iteration 33000, loss=0.7275450229644775\n",
      "Iteration 34000, loss=0.674625039100647\n",
      "Iteration 35000, loss=0.7191014289855957\n",
      "Iteration 36000, loss=0.7047718167304993\n",
      "Iteration 37000, loss=0.6783385276794434\n",
      "Iteration 38000, loss=0.7406435012817383\n",
      "Iteration 39000, loss=0.6971072554588318\n",
      "Iteration 40000, loss=0.7067972421646118\n",
      "Iteration 41000, loss=0.6735931634902954\n",
      "Iteration 42000, loss=0.7725293636322021\n",
      "Iteration 43000, loss=0.6804458498954773\n",
      "Iteration 44000, loss=0.7225874662399292\n",
      "Iteration 45000, loss=0.6951590776443481\n",
      "Iteration 46000, loss=0.6945837736129761\n",
      "Iteration 47000, loss=0.6906008124351501\n",
      "Iteration 48000, loss=0.692421019077301\n",
      "Iteration 49000, loss=0.6922036409378052\n",
      "Iteration 50000, loss=0.7242463827133179\n",
      "Iteration 51000, loss=0.6513475775718689\n",
      "Iteration 52000, loss=0.625730574131012\n",
      "Iteration 53000, loss=0.5857212543487549\n",
      "Iteration 54000, loss=0.710140585899353\n",
      "Iteration 55000, loss=0.6750032901763916\n",
      "Iteration 56000, loss=0.7214413285255432\n",
      "Iteration 57000, loss=0.6598848700523376\n",
      "Iteration 58000, loss=0.7065485119819641\n",
      "Iteration 59000, loss=0.6641916036605835\n",
      "Iteration 60000, loss=0.7337676882743835\n",
      "Iteration 61000, loss=0.6875883936882019\n",
      "Iteration 62000, loss=0.6960082650184631\n",
      "Iteration 63000, loss=0.7284761071205139\n",
      "Iteration 64000, loss=0.7317458987236023\n",
      "Iteration 65000, loss=0.5938879251480103\n",
      "Iteration 66000, loss=0.6502425670623779\n",
      "Iteration 67000, loss=0.7803026437759399\n",
      "Iteration 68000, loss=0.6228412389755249\n",
      "Iteration 69000, loss=0.916792631149292\n",
      "Iteration 70000, loss=0.7344203591346741\n",
      "Iteration 71000, loss=0.7595552206039429\n",
      "Iteration 72000, loss=0.7126474976539612\n",
      "Iteration 73000, loss=0.6688839197158813\n",
      "Iteration 74000, loss=0.6303976774215698\n",
      "Iteration 75000, loss=0.7354686856269836\n",
      "Iteration 76000, loss=0.7065485119819641\n",
      "Iteration 77000, loss=0.7203183174133301\n",
      "Iteration 78000, loss=0.6441811919212341\n",
      "Iteration 79000, loss=0.5932497978210449\n",
      "Iteration 80000, loss=0.7250194549560547\n",
      "Iteration 81000, loss=0.6043050289154053\n",
      "Iteration 82000, loss=0.6718212366104126\n",
      "Iteration 83000, loss=0.67704176902771\n",
      "Iteration 84000, loss=0.7536482810974121\n",
      "Iteration 85000, loss=0.6401726007461548\n",
      "Iteration 86000, loss=0.6907032132148743\n",
      "Iteration 87000, loss=0.8272563815116882\n",
      "Iteration 88000, loss=0.4220760464668274\n",
      "Iteration 89000, loss=0.6336137056350708\n",
      "Iteration 90000, loss=0.6534382700920105\n",
      "Iteration 91000, loss=0.7205651998519897\n",
      "Iteration 92000, loss=0.7309419512748718\n",
      "Iteration 93000, loss=0.7107746005058289\n",
      "Iteration 94000, loss=0.4941454529762268\n",
      "Iteration 95000, loss=5.084059715270996\n",
      "Iteration 96000, loss=0.8028109669685364\n",
      "Iteration 97000, loss=1.0021909475326538\n",
      "Iteration 98000, loss=0.6423450112342834\n",
      "Iteration 99000, loss=0.6367618441581726\n",
      "Iteration 100000, loss=0.2302820086479187\n",
      "Iteration 101000, loss=0.6342841386795044\n",
      "Iteration 102000, loss=0.6073462963104248\n",
      "Iteration 103000, loss=0.7621872425079346\n",
      "Iteration 104000, loss=0.5556299686431885\n",
      "Iteration 105000, loss=0.8804144859313965\n",
      "Iteration 106000, loss=0.06075071543455124\n",
      "Iteration 107000, loss=0.6659519672393799\n",
      "Iteration 108000, loss=0.7504560947418213\n",
      "Iteration 109000, loss=0.8448849320411682\n",
      "Iteration 110000, loss=0.7859388589859009\n",
      "Iteration 111000, loss=0.014447725377976894\n",
      "Iteration 112000, loss=0.5086895823478699\n",
      "Iteration 113000, loss=1.3553588390350342\n",
      "Iteration 114000, loss=2.2560248374938965\n",
      "Iteration 115000, loss=0.10114309191703796\n",
      "Iteration 116000, loss=0.6935107111930847\n",
      "Iteration 117000, loss=0.5256791114807129\n",
      "Iteration 118000, loss=0.5863522291183472\n",
      "Iteration 119000, loss=0.546002984046936\n",
      "Iteration 120000, loss=0.564563512802124\n",
      "Iteration 121000, loss=0.9024523496627808\n",
      "Iteration 122000, loss=0.8036972284317017\n",
      "Iteration 123000, loss=0.5515384674072266\n",
      "Iteration 124000, loss=0.6017042398452759\n",
      "Iteration 125000, loss=0.3358705937862396\n",
      "Iteration 126000, loss=0.44839149713516235\n",
      "Iteration 127000, loss=0.6110062003135681\n",
      "Iteration 128000, loss=0.3053462505340576\n",
      "Iteration 129000, loss=0.0036759504582732916\n",
      "Iteration 130000, loss=0.9778699278831482\n",
      "Iteration 131000, loss=0.606407642364502\n",
      "Iteration 132000, loss=0.5405638813972473\n",
      "Iteration 133000, loss=0.4623429775238037\n",
      "Iteration 134000, loss=1.024584412574768\n",
      "Iteration 135000, loss=0.6917951107025146\n",
      "Iteration 136000, loss=0.02268916368484497\n",
      "Iteration 137000, loss=0.7394945621490479\n",
      "Iteration 138000, loss=0.899884819984436\n",
      "Iteration 139000, loss=0.5536583065986633\n",
      "Iteration 140000, loss=0.003585320431739092\n",
      "Iteration 141000, loss=0.5171408653259277\n",
      "Iteration 142000, loss=0.4703511893749237\n",
      "Iteration 143000, loss=0.6594134569168091\n",
      "Iteration 144000, loss=0.5289124250411987\n",
      "Iteration 145000, loss=0.9788148403167725\n",
      "Iteration 146000, loss=0.08664000034332275\n",
      "Iteration 147000, loss=0.5465000867843628\n",
      "Iteration 148000, loss=0.9186307787895203\n",
      "Iteration 149000, loss=0.0039821164682507515\n",
      "Iteration 150000, loss=0.47061681747436523\n",
      "Iteration 151000, loss=1.109143614768982\n",
      "Iteration 152000, loss=0.5899946689605713\n",
      "Iteration 153000, loss=0.4937790334224701\n",
      "Iteration 154000, loss=0.5610790848731995\n",
      "Iteration 155000, loss=0.0205962173640728\n",
      "Iteration 156000, loss=1.064418077468872\n",
      "Iteration 157000, loss=3.5762778338721546e-07\n",
      "Iteration 158000, loss=0.07696118950843811\n",
      "Iteration 159000, loss=0.49266326427459717\n",
      "Iteration 160000, loss=0.16367030143737793\n",
      "Iteration 161000, loss=1.0280218124389648\n",
      "Iteration 162000, loss=0.42770856618881226\n",
      "Iteration 163000, loss=0.5240331888198853\n",
      "Iteration 164000, loss=1.192093321833454e-07\n",
      "Iteration 165000, loss=0.7498500943183899\n",
      "Iteration 166000, loss=0.8150602579116821\n",
      "Iteration 167000, loss=0.5518508553504944\n",
      "Iteration 168000, loss=1.1049655675888062\n",
      "Iteration 169000, loss=0.1442764699459076\n",
      "Iteration 170000, loss=0.9285154342651367\n",
      "Iteration 171000, loss=0.6554949879646301\n",
      "Iteration 172000, loss=0.3885171711444855\n",
      "Iteration 173000, loss=0.2418215423822403\n",
      "Iteration 174000, loss=0.4041697382926941\n",
      "Iteration 175000, loss=0.6736145615577698\n",
      "Iteration 176000, loss=0.662812352180481\n",
      "Iteration 177000, loss=0.9660684466362\n",
      "Iteration 178000, loss=0.11496403068304062\n",
      "Iteration 179000, loss=0.46498769521713257\n",
      "Iteration 180000, loss=0.015560666099190712\n",
      "Iteration 181000, loss=0.5214304327964783\n",
      "Iteration 182000, loss=0.41095423698425293\n",
      "Iteration 183000, loss=0.4581013321876526\n",
      "Iteration 184000, loss=0.8462695479393005\n",
      "Iteration 185000, loss=0.26281827688217163\n",
      "Iteration 186000, loss=0.7901214361190796\n",
      "Iteration 187000, loss=0.4066651463508606\n",
      "Iteration 188000, loss=0.720073401927948\n",
      "Iteration 189000, loss=2.147963523864746\n",
      "Iteration 190000, loss=0.4025864005088806\n",
      "Iteration 191000, loss=0.7683447599411011\n",
      "Iteration 192000, loss=0.5139145255088806\n",
      "Iteration 193000, loss=0.4295864403247833\n",
      "Iteration 194000, loss=0.4407503604888916\n",
      "Iteration 195000, loss=0.734359622001648\n",
      "Iteration 196000, loss=0.3868456780910492\n",
      "Iteration 197000, loss=0.6125799417495728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 198000, loss=0.5163780450820923\n",
      "Iteration 199000, loss=0.9668267369270325\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(os.path.join(os.path.abspath('../binaries'),\"models\"),\"classification_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

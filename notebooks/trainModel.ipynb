{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, dot\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"vocab_size\",\"labels\",\"word_target\",\"word_context\",\"window_size\"]\n",
    "for name in names:\n",
    "    with open(os.path.join(os.path.join(os.path.abspath('../binaries'),\"pickles\"),name+\".pickle\"), \"rb\") as f:\n",
    "        globals()[name]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim=300\n",
    "epochs=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW1129 10:33:38.428618 24760 deprecation_wrapper.py:119] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nW1129 10:33:38.449561 24760 deprecation_wrapper.py:119] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n"
    }
   ],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W1129 10:33:38.755780 24760 deprecation_wrapper.py:119] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\n"
    }
   ],
   "source": [
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([target,context],axes=1,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n  after removing the cwd from sys.path.\nW1129 10:33:39.089839 24760 deprecation_wrapper.py:119] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nW1129 10:33:39.101778 24760 deprecation_wrapper.py:119] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n\nW1129 10:33:39.104770 24760 deprecation.py:323] From C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
    }
   ],
   "source": [
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\nIteration 538000, loss=1.192093321833454e-07\nIteration 539000, loss=1.192093321833454e-07\nIteration 540000, loss=0.49969592690467834\nIteration 541000, loss=1.5497212189075071e-06\nIteration 542000, loss=0.06890684366226196\nIteration 543000, loss=0.13895699381828308\nIteration 544000, loss=0.19937723875045776\nIteration 545000, loss=1.3841215372085571\nIteration 546000, loss=2.4229722023010254\nIteration 547000, loss=1.192093321833454e-07\nIteration 548000, loss=0.014032488688826561\nIteration 549000, loss=0.060790758579969406\nIteration 550000, loss=1.192093321833454e-07\nIteration 551000, loss=0.11824224144220352\nIteration 552000, loss=1.192093321833454e-07\nIteration 553000, loss=0.3683505356311798\nIteration 554000, loss=1.192093321833454e-07\nIteration 555000, loss=0.011266155168414116\nIteration 556000, loss=1.2042778730392456\nIteration 557000, loss=0.1248830035328865\nIteration 558000, loss=0.18558184802532196\nIteration 559000, loss=0.1895647495985031\nIteration 560000, loss=0.00011826265108538792\nIteration 561000, loss=0.05138489231467247\nIteration 562000, loss=1.192093321833454e-07\nIteration 563000, loss=0.13734957575798035\nIteration 564000, loss=0.11366501450538635\nIteration 565000, loss=0.1235743910074234\nIteration 566000, loss=1.192093321833454e-07\nIteration 567000, loss=0.00019689348118845373\nIteration 568000, loss=0.14855854213237762\nIteration 569000, loss=0.0017064461717382073\nIteration 570000, loss=0.1137983426451683\nIteration 571000, loss=0.002231702208518982\nIteration 572000, loss=0.08201929926872253\nIteration 573000, loss=3.7235913276672363\nIteration 574000, loss=5.364431672205683e-06\nIteration 575000, loss=0.12188486754894257\nIteration 576000, loss=1.192093321833454e-07\nIteration 577000, loss=0.08412652462720871\nIteration 578000, loss=1.1682582226058003e-05\nIteration 579000, loss=1.192093321833454e-07\nIteration 580000, loss=0.07843133062124252\nIteration 581000, loss=0.2913248836994171\nIteration 582000, loss=0.1238495409488678\nIteration 583000, loss=0.000848414667416364\nIteration 584000, loss=0.1009017750620842\nIteration 585000, loss=0.00029490754241123796\nIteration 586000, loss=0.21172045171260834\nIteration 587000, loss=2.3841855067985307e-07\nIteration 588000, loss=0.1893717646598816\nIteration 589000, loss=3.5762778338721546e-07\nIteration 590000, loss=0.11956033855676651\nIteration 591000, loss=0.16228613257408142\nIteration 592000, loss=0.12578143179416656\nIteration 593000, loss=1.192093321833454e-07\nIteration 594000, loss=0.0001929587742779404\nIteration 595000, loss=1.192093321833454e-07\nIteration 596000, loss=0.04770428314805031\nIteration 597000, loss=1.192093321833454e-07\nIteration 598000, loss=0.1492755115032196\nIteration 599000, loss=1.192093321833454e-07\nIteration 600000, loss=0.31887948513031006\nIteration 601000, loss=0.017780205234885216\nIteration 602000, loss=2.3227803707122803\nIteration 603000, loss=0.13064201176166534\nIteration 604000, loss=8.813547134399414\nIteration 605000, loss=0.4241325855255127\nIteration 606000, loss=0.04873867705464363\nIteration 607000, loss=0.5615247488021851\nIteration 608000, loss=1.192093321833454e-07\nIteration 609000, loss=2.315387010574341\nIteration 610000, loss=0.11751192808151245\nIteration 611000, loss=0.2609907388687134\nIteration 612000, loss=0.17715846002101898\nIteration 613000, loss=0.09468787163496017\nIteration 614000, loss=0.15758748352527618\nIteration 615000, loss=1.192093321833454e-07\nIteration 616000, loss=1.192093321833454e-07\nIteration 617000, loss=1.192093321833454e-07\nIteration 618000, loss=0.3179745078086853\nIteration 619000, loss=0.16164252161979675\nIteration 620000, loss=0.16675251722335815\nIteration 621000, loss=0.16133680939674377\nIteration 622000, loss=0.24025678634643555\nIteration 623000, loss=0.631483256816864\nIteration 624000, loss=1.9507877826690674\nIteration 625000, loss=0.10070805251598358\nIteration 626000, loss=0.09956874698400497\nIteration 627000, loss=2.061842679977417\nIteration 628000, loss=1.192093321833454e-07\nIteration 629000, loss=0.3961966931819916\nIteration 630000, loss=0.6241930723190308\nIteration 631000, loss=1.307907223701477\nIteration 632000, loss=0.0684444010257721\nIteration 633000, loss=2.741816388152074e-06\nIteration 634000, loss=0.38925570249557495\nIteration 635000, loss=1.3813996315002441\nIteration 636000, loss=0.13886600732803345\nIteration 637000, loss=0.09646444767713547\nIteration 638000, loss=0.5669792890548706\nIteration 639000, loss=3.275324821472168\nIteration 640000, loss=1.192093321833454e-07\nIteration 641000, loss=1.7355318069458008\nIteration 642000, loss=0.08936718851327896\nIteration 643000, loss=7.152557373046875e-07\nIteration 644000, loss=0.12439364939928055\nIteration 645000, loss=1.192093321833454e-07\nIteration 646000, loss=0.1670222282409668\nIteration 647000, loss=1.192093321833454e-07\nIteration 648000, loss=0.25940749049186707\nIteration 649000, loss=0.007465564180165529\nIteration 650000, loss=0.14624378085136414\nIteration 651000, loss=1.382837763230782e-05\nIteration 652000, loss=0.13124729692935944\nIteration 653000, loss=0.04605351388454437\nIteration 654000, loss=0.004103784449398518\nIteration 655000, loss=0.0014975545927882195\nIteration 656000, loss=0.044681716710329056\nIteration 657000, loss=3.0430548191070557\nIteration 658000, loss=1.192093321833454e-07\nIteration 659000, loss=1.7040287256240845\nIteration 660000, loss=0.3674555718898773\nIteration 661000, loss=0.009494507685303688\nIteration 662000, loss=1.192093321833454e-07\nIteration 663000, loss=0.21309275925159454\nIteration 664000, loss=4.768372718899627e-07\nIteration 665000, loss=0.126164510846138\nIteration 666000, loss=1.192093321833454e-07\nIteration 667000, loss=0.15562604367733002\nIteration 668000, loss=0.10971526056528091\nIteration 669000, loss=2.125624179840088\nIteration 670000, loss=1.192093321833454e-07\nIteration 671000, loss=1.192093321833454e-07\nIteration 672000, loss=1.3784711360931396\nIteration 673000, loss=1.192093321833454e-07\nIteration 674000, loss=1.192093321833454e-07\nIteration 675000, loss=0.5035543441772461\nIteration 676000, loss=0.04199327528476715\nIteration 677000, loss=0.33025550842285156\nIteration 678000, loss=0.6901365518569946\nIteration 679000, loss=1.192093321833454e-07\nIteration 680000, loss=0.19376501441001892\nIteration 681000, loss=0.10795847326517105\nIteration 682000, loss=0.14932650327682495\nIteration 683000, loss=0.12340238690376282\nIteration 684000, loss=1.192093321833454e-07\nIteration 685000, loss=1.192093321833454e-07\nIteration 686000, loss=0.17105360329151154\nIteration 687000, loss=0.1966148167848587\nIteration 688000, loss=0.02104571834206581\nIteration 689000, loss=0.15199846029281616\nIteration 690000, loss=1.192093321833454e-07\nIteration 691000, loss=1.192093321833454e-07\nIteration 692000, loss=0.1424439549446106\nIteration 693000, loss=1.192093321833454e-07\nIteration 694000, loss=0.2208958864212036\nIteration 695000, loss=2.744953155517578\nIteration 696000, loss=1.492250919342041\nIteration 697000, loss=1.192093321833454e-07\nIteration 698000, loss=0.15517038106918335\nIteration 699000, loss=0.8888741731643677\nIteration 700000, loss=0.013387302868068218\nIteration 701000, loss=0.121494360268116\nIteration 702000, loss=0.016650063917040825\nIteration 703000, loss=0.00014592273510061204\nIteration 704000, loss=1.192093321833454e-07\nIteration 705000, loss=1.706322431564331\nIteration 706000, loss=15.942384719848633\nIteration 707000, loss=0.16107937693595886\nIteration 708000, loss=0.1611456722021103\nIteration 709000, loss=1.4913549423217773\nIteration 710000, loss=0.20093293488025665\nIteration 711000, loss=0.17717096209526062\nIteration 712000, loss=0.7032637596130371\nIteration 713000, loss=0.1961384266614914\nIteration 714000, loss=1.192093321833454e-07\nIteration 715000, loss=0.14189748466014862\nIteration 716000, loss=1.7218283414840698\nIteration 717000, loss=1.192093321833454e-07\nIteration 718000, loss=0.01591106504201889\nIteration 719000, loss=0.9125323295593262\nIteration 720000, loss=0.1914544552564621\nIteration 721000, loss=1.192093321833454e-07\nIteration 722000, loss=0.1808663159608841\nIteration 723000, loss=0.18551617860794067\nIteration 724000, loss=1.192093321833454e-07\nIteration 725000, loss=0.18368928134441376\nIteration 726000, loss=1.192093321833454e-07\nIteration 727000, loss=0.3474990725517273\nIteration 728000, loss=0.4977448582649231\nIteration 729000, loss=1.40242600440979\nIteration 730000, loss=5.602852979791351e-06\nIteration 731000, loss=0.08605135977268219\nIteration 732000, loss=0.07675077766180038\nIteration 733000, loss=0.17129835486412048\nIteration 734000, loss=1.192093321833454e-07\nIteration 735000, loss=0.08900754153728485\nIteration 736000, loss=0.17347463965415955\nIteration 737000, loss=0.4864898920059204\nIteration 738000, loss=1.497960090637207\nIteration 739000, loss=1.192093321833454e-07\nIteration 740000, loss=0.18953275680541992\nIteration 741000, loss=1.192093321833454e-07\nIteration 742000, loss=1.192093321833454e-07\nIteration 743000, loss=0.17465823888778687\nIteration 744000, loss=4.048778057098389\nIteration 745000, loss=0.17792733013629913\nIteration 746000, loss=1.192093321833454e-07\nIteration 747000, loss=0.07786647975444794\nIteration 748000, loss=1.1934843063354492\nIteration 749000, loss=1.192093321833454e-07\nIteration 750000, loss=0.11871527135372162\nIteration 751000, loss=0.13785894215106964\nIteration 752000, loss=5.7937380915973336e-05\nIteration 753000, loss=1.192093321833454e-07\nIteration 754000, loss=1.192093321833454e-07\nIteration 755000, loss=0.14164471626281738\nIteration 756000, loss=1.982041835784912\nIteration 757000, loss=0.19320930540561676\nIteration 758000, loss=1.1444150914030615e-05\nIteration 759000, loss=0.045264776796102524\nIteration 760000, loss=0.1675800085067749\nIteration 761000, loss=1.192093321833454e-07\nIteration 762000, loss=1.192093321833454e-07\nIteration 763000, loss=2.7776155548053794e-05\nIteration 764000, loss=0.3957953453063965\nIteration 765000, loss=0.06939156353473663\nIteration 766000, loss=0.0509246364235878\nIteration 767000, loss=1.192093321833454e-07\nIteration 768000, loss=0.1437210887670517\nIteration 769000, loss=0.2856596112251282\nIteration 770000, loss=0.09432072937488556\nIteration 771000, loss=0.1366308480501175\nIteration 772000, loss=2.336792469024658\nIteration 773000, loss=0.16175048053264618\nIteration 774000, loss=0.1549050360918045\nIteration 775000, loss=0.13090164959430695\nIteration 776000, loss=0.16467779874801636\nIteration 777000, loss=0.11147304624319077\nIteration 778000, loss=0.12115157395601273\nIteration 779000, loss=1.192093321833454e-07\nIteration 780000, loss=0.1376064121723175\nIteration 781000, loss=0.15564411878585815\nIteration 782000, loss=0.06847315281629562\nIteration 783000, loss=1.4710015058517456\nIteration 784000, loss=0.26944780349731445\nIteration 785000, loss=0.16592401266098022\nIteration 786000, loss=1.7895584106445312\nIteration 787000, loss=0.14742396771907806\nIteration 788000, loss=0.12900251150131226\nIteration 789000, loss=3.2884058952331543\nIteration 790000, loss=1.192093321833454e-07\nIteration 791000, loss=0.1195598617196083\nIteration 792000, loss=0.14812320470809937\nIteration 793000, loss=0.6444008350372314\nIteration 794000, loss=1.8978954553604126\nIteration 795000, loss=0.14830613136291504\nIteration 796000, loss=0.12868520617485046\nIteration 797000, loss=0.15837670862674713\nIteration 798000, loss=0.17267784476280212\nIteration 799000, loss=0.1703539341688156\nIteration 800000, loss=0.09029244631528854\nIteration 801000, loss=2.4255659580230713\nIteration 802000, loss=2.058016300201416\nIteration 803000, loss=0.46060246229171753\nIteration 804000, loss=1.192093321833454e-07\nIteration 805000, loss=0.13256047666072845\nIteration 806000, loss=0.18397732079029083\nIteration 807000, loss=0.0943908616900444\nIteration 808000, loss=0.13875359296798706\nIteration 809000, loss=0.002400713972747326\nIteration 810000, loss=0.16152417659759521\nIteration 811000, loss=0.14693252742290497\nIteration 812000, loss=0.18741147220134735\nIteration 813000, loss=1.9066030979156494\nIteration 814000, loss=3.0544474124908447\nIteration 815000, loss=0.09237457066774368\nIteration 816000, loss=0.14555878937244415\nIteration 817000, loss=0.08996741473674774\nIteration 818000, loss=1.192093321833454e-07\nIteration 819000, loss=0.10234730690717697\nIteration 820000, loss=1.192093321833454e-07\nIteration 821000, loss=1.192093321833454e-07\nIteration 822000, loss=1.192093321833454e-07\nIteration 823000, loss=0.11600708216428757\nIteration 824000, loss=0.12494044750928879\nIteration 825000, loss=1.5020475984783843e-05\nIteration 826000, loss=1.192093321833454e-07\nIteration 827000, loss=1.192093321833454e-07\nIteration 828000, loss=0.17376919090747833\nIteration 829000, loss=0.050897277891635895\nIteration 830000, loss=1.192093321833454e-07\nIteration 831000, loss=0.13698934018611908\nIteration 832000, loss=0.02336840331554413\nIteration 833000, loss=0.026915179565548897\nIteration 834000, loss=1.192093321833454e-07\nIteration 835000, loss=0.19365064799785614\nIteration 836000, loss=2.7315263748168945\nIteration 837000, loss=0.185433030128479\nIteration 838000, loss=0.09438302367925644\nIteration 839000, loss=0.04253369942307472\nIteration 840000, loss=0.14556515216827393\nIteration 841000, loss=0.14876823127269745\nIteration 842000, loss=0.23155370354652405\nIteration 843000, loss=9.79948163148947e-05\nIteration 844000, loss=2.1146039962768555\nIteration 845000, loss=0.00036436994560062885\nIteration 846000, loss=1.192093321833454e-07\nIteration 847000, loss=0.11766710877418518\nIteration 848000, loss=0.051833152770996094\nIteration 849000, loss=0.14229628443717957\nIteration 850000, loss=0.01765010692179203\nIteration 851000, loss=0.15227945148944855\nIteration 852000, loss=0.042015694081783295\nIteration 853000, loss=0.2720775604248047\nIteration 854000, loss=0.09164866805076599\nIteration 855000, loss=3.559692859649658\nIteration 856000, loss=0.2490922212600708\nIteration 857000, loss=2.3952972888946533\nIteration 858000, loss=0.4178689122200012\nIteration 859000, loss=4.768372718899627e-07\nIteration 860000, loss=0.022303011268377304\nIteration 861000, loss=0.18817195296287537\nIteration 862000, loss=1.192093321833454e-07\nIteration 863000, loss=3.0585150718688965\nIteration 864000, loss=0.1439087986946106\nIteration 865000, loss=0.21773545444011688\nIteration 866000, loss=1.3609404563903809\nIteration 867000, loss=0.6684625744819641\nIteration 868000, loss=0.02538924850523472\nIteration 869000, loss=1.192093321833454e-07\nIteration 870000, loss=1.192093321833454e-07\nIteration 871000, loss=0.12966424226760864\nIteration 872000, loss=0.22508785128593445\nIteration 873000, loss=1.192093321833454e-07\nIteration 874000, loss=1.6454615592956543\nIteration 875000, loss=0.1644088178873062\nIteration 876000, loss=0.46062275767326355\nIteration 877000, loss=0.018393440172076225\nIteration 878000, loss=0.0834752693772316\nIteration 879000, loss=1.192093321833454e-07\nIteration 880000, loss=0.16543731093406677\nIteration 881000, loss=1.192093321833454e-07\nIteration 882000, loss=2.1457688035297906e-06\nIteration 883000, loss=1.192093321833454e-07\nIteration 884000, loss=1.7674620151519775\nIteration 885000, loss=1.192093321833454e-07\nIteration 886000, loss=1.7362515926361084\nIteration 887000, loss=0.444308340549469\nIteration 888000, loss=1.192093321833454e-07\nIteration 889000, loss=1.192093321833454e-07\nIteration 890000, loss=0.1833922415971756\nIteration 891000, loss=1.192093321833454e-07\nIteration 892000, loss=1.192093321833454e-07\nIteration 893000, loss=0.1733504980802536\nIteration 894000, loss=0.07544886320829391\nIteration 895000, loss=0.08128126710653305\nIteration 896000, loss=1.192093321833454e-07\nIteration 897000, loss=1.2170366048812866\nIteration 898000, loss=0.1252679079771042\nIteration 899000, loss=0.20287713408470154\nIteration 900000, loss=0.053747694939374924\nIteration 901000, loss=9.86703872680664\nIteration 902000, loss=0.18683119118213654\nIteration 903000, loss=0.2549840211868286\nIteration 904000, loss=1.192093321833454e-07\nIteration 905000, loss=0.14497442543506622\nIteration 906000, loss=0.10659638047218323\nIteration 907000, loss=0.16270029544830322\nIteration 908000, loss=1.192093321833454e-07\nIteration 909000, loss=1.192093321833454e-07\nIteration 910000, loss=0.3108993172645569\nIteration 911000, loss=0.4159080684185028\nIteration 912000, loss=0.18993930518627167\nIteration 913000, loss=0.12365056574344635\nIteration 914000, loss=5.785881519317627\nIteration 915000, loss=0.15850959718227386\nIteration 916000, loss=0.12388262897729874\nIteration 917000, loss=0.10201037675142288\nIteration 918000, loss=0.023093003779649734\nIteration 919000, loss=1.192093321833454e-07\nIteration 920000, loss=2.470658302307129\nIteration 921000, loss=2.1543097496032715\nIteration 922000, loss=0.14010821282863617\nIteration 923000, loss=0.13915008306503296\nIteration 924000, loss=0.00016499929188285023\nIteration 925000, loss=0.16245080530643463\nIteration 926000, loss=1.192093321833454e-07\nIteration 927000, loss=0.09952002018690109\nIteration 928000, loss=0.0013927972177043557\nIteration 929000, loss=0.06020713225007057\nIteration 930000, loss=5.602852979791351e-06\nIteration 931000, loss=0.18053871393203735\nIteration 932000, loss=1.837422251701355\nIteration 933000, loss=1.192093321833454e-07\nIteration 934000, loss=0.019239502027630806\nIteration 935000, loss=0.26747626066207886\nIteration 936000, loss=0.0006676161428913474\nIteration 937000, loss=0.06870342791080475\nIteration 938000, loss=1.192093321833454e-07\nIteration 939000, loss=0.18407703936100006\nIteration 940000, loss=0.16761210560798645\nIteration 941000, loss=0.15015628933906555\nIteration 942000, loss=0.009036467410624027\nIteration 943000, loss=0.0001001408018055372\nIteration 944000, loss=0.5510426759719849\nIteration 945000, loss=0.1642327457666397\nIteration 946000, loss=0.5320150852203369\nIteration 947000, loss=0.14340075850486755\nIteration 948000, loss=1.192093321833454e-07\nIteration 949000, loss=0.0024345319252461195\nIteration 950000, loss=1.192093321833454e-07\nIteration 951000, loss=2.026559741352685e-06\nIteration 952000, loss=0.003499724203720689\nIteration 953000, loss=0.146469384431839\nIteration 954000, loss=1.192093321833454e-07\nIteration 955000, loss=7.391000963252736e-06\nIteration 956000, loss=0.25966912508010864\nIteration 957000, loss=0.09327376633882523\nIteration 958000, loss=1.757064938545227\nIteration 959000, loss=0.14299945533275604\nIteration 960000, loss=0.15251074731349945\nIteration 961000, loss=3.5762843708653236e-06\nIteration 962000, loss=0.0799243301153183\nIteration 963000, loss=1.192093321833454e-07\nIteration 964000, loss=0.13163280487060547\nIteration 965000, loss=1.7884551286697388\nIteration 966000, loss=0.17493900656700134\nIteration 967000, loss=0.17283090949058533\nIteration 968000, loss=1.192093321833454e-07\nIteration 969000, loss=1.8789201974868774\nIteration 970000, loss=0.17496754229068756\nIteration 971000, loss=0.29651400446891785\nIteration 972000, loss=1.192093321833454e-07\nIteration 973000, loss=0.21966835856437683\nIteration 974000, loss=1.192093321833454e-07\nIteration 975000, loss=0.06647945195436478\nIteration 976000, loss=1.1026513576507568\nIteration 977000, loss=1.192093321833454e-07\nIteration 978000, loss=0.12957783043384552\nIteration 979000, loss=1.192093321833454e-07\nIteration 980000, loss=0.18635612726211548\nIteration 981000, loss=0.16380220651626587\nIteration 982000, loss=0.3389963209629059\nIteration 983000, loss=0.1528148502111435\nIteration 984000, loss=0.13655366003513336\nIteration 985000, loss=0.17704389989376068\nIteration 986000, loss=0.1217949390411377\nIteration 987000, loss=1.7299929857254028\nIteration 988000, loss=0.189449280500412\nIteration 989000, loss=0.31483256816864014\nIteration 990000, loss=1.192093321833454e-07\nIteration 991000, loss=0.1884087771177292\nIteration 992000, loss=2.9528021812438965\nIteration 993000, loss=1.7123323678970337\nIteration 994000, loss=3.7790046917507425e-05\nIteration 995000, loss=1.8984315395355225\nIteration 996000, loss=0.12378237396478653\nIteration 997000, loss=2.78193998336792\nIteration 998000, loss=0.1446465700864792\nIteration 999000, loss=0.007727359887212515\n"
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "        model.save(os.path.join(os.path.join(os.path.abspath('../binaries'),\"models\"),\"classification_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(os.path.join(os.path.abspath('../binaries'),\"models\"),\"classification_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
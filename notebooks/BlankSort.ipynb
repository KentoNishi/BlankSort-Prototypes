{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext as ft\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\yoshi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n  \"C extension not loaded, training will be slow. \"\n"
    }
   ],
   "source": [
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "model=ft.load_facebook_model(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3\n",
    "stemmer = nltk.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(text,top=5,collapse=True):\n",
    "    text=text.lower()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model.wv.vocab):\n",
    "            filtered_words.append(word)\n",
    "            counts.setdefault(word,0)\n",
    "            counts[word]+=1\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=model.wv.similarity(filtered_words[i],filtered_words[j])\n",
    "            similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        phrases=[]\n",
    "        bagOfWords=set()\n",
    "        for word in filteredWordScores[:(len(filtered_words)//3)]:\n",
    "            bagOfWords.add(word[0])\n",
    "        phrase=\"\"\n",
    "        for word in tokens:\n",
    "            if word in stops:\n",
    "                if(len(phrase)>0):\n",
    "                    phrase+=word+\" \"\n",
    "            elif word in bagOfWords:\n",
    "                phrase+=word+\" \"\n",
    "            else:\n",
    "                if(phrase[:len(phrase)-1]!=\"\"):\n",
    "                    phrases.append(phrase[:len(phrase)-1])\n",
    "                    phrase=\"\"\n",
    "        return phrases\n",
    "    else:\n",
    "        return filteredWordScores[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['fasttext',\n 'library for learning',\n 'word',\n 'classification created',\n 'facebook',\n 'model',\n 'learning',\n 'learning',\n 'facebook',\n 'available pretrained',\n 'fasttext',\n 'network for word embedding']"
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(\"\"\"\n",
    "fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. fastText uses a neural network for word embedding.\n",
    "\"\"\",5,True)"
   ]
  }
 ]
}
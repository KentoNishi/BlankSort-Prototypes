{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext as ft\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import operator\n",
    "import re\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "if \"model\" not in globals():\n",
    "    model=ft.load_facebook_vectors(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3\n",
    "stemmer = nltk.wordnet.WordNetLemmatizer()\n",
    "porterStemmer=PorterStemmer()\n",
    "# https://github.com/Alir3z4/stop-words\n",
    "stops=set(line.strip() for line in open(os.path.join(os.getcwd(),\"binaries/data/stopwords-en.txt\"),encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(text,top=None,collapse=False):\n",
    "    text=text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if porterStemmer.stem(word) not in stops and word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model.vocab):\n",
    "            filtered_words.append(filtered)\n",
    "            counts.setdefault(filtered,0)\n",
    "            counts[filtered]+=1\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=model.similarity(filtered_words[i],filtered_words[j])\n",
    "            similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        if(top==None):\n",
    "            top=min(10,len(filtered_words)//3)\n",
    "        #print(scores)\n",
    "        #print(filteredWordScores)\n",
    "        phraseScores=collapsePhrases(tokens,scores,filteredWordScores[:top],cleaned_words)\n",
    "        phraseScores=diversifyResults([x[0] for x in phraseScores])\n",
    "        return phraseScores[:top]\n",
    "    else:\n",
    "        if(top==None):\n",
    "            top=len(filteredWordScores)//3\n",
    "        return filteredWordScores[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversifyResults(phrases):\n",
    "    phrases=[[phrase,0.0] for phrase in phrases]\n",
    "    for phrase1 in range(len(phrases)):\n",
    "        for phrase2 in range(len(phrases)):\n",
    "            if(phrase1!=phrase2):\n",
    "                score=model.similarity(phrases[phrase1][0],phrases[phrase2][0])\n",
    "                score=(score+1)/2.0\n",
    "                phrases[phrase1][1]+=score\n",
    "                phrases[phrase2][1]+=score\n",
    "    phrases=[[phrases[i][0],phrases[i][1]/(len(phrases)-1)] for i in range(len(phrases))]\n",
    "    phrases=sorted(phrases, key = lambda x: x[1],reverse=True)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePhrases(tokens,scores,filteredWordScores,cleaned_words,sort=False):\n",
    "    dictionary=set()\n",
    "    for word in cleaned_words:\n",
    "        dictionary.add(word)\n",
    "    phrases=dict()\n",
    "    bagOfWords=set()\n",
    "    for word in filteredWordScores:\n",
    "        bagOfWords.add(word[0])\n",
    "    phrase=\"\"\n",
    "    totalScore=0\n",
    "    wordCount=0\n",
    "    i=0\n",
    "    def reset():\n",
    "        nonlocal wordCount,phrase,totalScore\n",
    "        if(wordCount>0):\n",
    "            phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "            totalScore=0\n",
    "            wordCount=0\n",
    "            phrase=\"\"\n",
    "    while i in range(len(tokens)):\n",
    "        word=stemmer.lemmatize(tokens[i])\n",
    "        if (word in stops or tokens[i] in stops) and phrase!=\"\":\n",
    "            j=i+1\n",
    "            while j<len(tokens) and stemmer.lemmatize(tokens[j]) not in dictionary:\n",
    "                j+=1\n",
    "            if(j<len(tokens) and stemmer.lemmatize(tokens[j]) in bagOfWords):\n",
    "                for k in range(i,j):\n",
    "                    phrase+=tokens[k]+\" \"\n",
    "                i=j-1\n",
    "            else:\n",
    "                reset()\n",
    "        elif word in bagOfWords:\n",
    "            totalScore+=scores[word]\n",
    "            wordCount+=1\n",
    "            phrase+=tokens[i]+\" \"\n",
    "        else:\n",
    "            reset()\n",
    "        i+=1\n",
    "    phraseScores=list(map(list, phrases.items()))\n",
    "    if sort:\n",
    "        phraseScores=sorted(phraseScores, key = lambda x: x[1])#len(x[0].split()))#,reverse=True)\n",
    "    return phraseScores"
   ]
  }
 ]
}
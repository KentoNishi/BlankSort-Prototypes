{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext as ft\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlankSort(BlankSort):\n",
    "    def __init__(self):\n",
    "        self._loadData()\n",
    "    def _loadData(self):\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('stopwords')\n",
    "        if \"model\" not in dir(self):\n",
    "            # https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "            type(self)._model=ft.load_facebook_vectors(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))\n",
    "        type(self)._window_size=3\n",
    "        type(self)._lemmatizer=nltk.wordnet.WordNetLemmatizer()\n",
    "        type(self)._porter=nltk.stem.porter.PorterStemmer()\n",
    "        # https://github.com/Alir3z4/stop-words\n",
    "        type(self)._stops=set(line.strip() for line in open(os.path.join(os.getcwd(),\"binaries/data/stopwords-en.txt\"),encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlankSort(BlankSort):\n",
    "    def rank(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "blanksort=BlankSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blanksort.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def rank(text,top=None,collapse=False):\n",
    "    text=text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if porterStemmer.stem(word) not in stops and word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model.vocab):\n",
    "            filtered_words.append(filtered)\n",
    "            counts.setdefault(filtered,0)\n",
    "            counts[filtered]+=1\n",
    "    global similarityMatrix\n",
    "    similarityMatrix=numpy.full((len(filtered_words),len(filtered_words)),None)\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=0.0\n",
    "            if similarityMatrix[i][j]!=None:\n",
    "                similarity_score=similarityMatrix[i][j]\n",
    "            else:\n",
    "                similarity_score=model.similarity(filtered_words[i],filtered_words[j])\n",
    "                similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "            similarityMatrix[i][j]=similarity_score\n",
    "            similarityMatrix[j][i]=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        if(top==None):\n",
    "            top=max(5,len(filtered_words)//3)\n",
    "        print(\"Phrase collection is disabled.\")\n",
    "        #print(scores)\n",
    "        #print(filteredWordScores)\n",
    "        #filteredWordScores=removeSimilarWords(filteredWordScores)\n",
    "        # phraseScores=collapsePhrases(tokens,scores,filteredWordScores[:top],cleaned_words)\n",
    "        #phraseScores=diversifyResults([x[0] for x in phraseScores])\n",
    "        # return phraseScores[:top]\n",
    "        return filteredWordScores[:top]\n",
    "    else:\n",
    "        if(top==None):\n",
    "            top=len(filteredWordScores)//3\n",
    "        return filteredWordScores[:top]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def removeSimilarWords(scores,simScore=0.8):\n",
    "    N=len(scores)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if(i!=j and model.similarity(scores[i][0],scores[j][0])>=simScore):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/tech-that-works/maximal-marginal-relevance-to-rerank-results-in-unsupervised-keyphrase-extraction-22d95015c7c5\n",
    "def maximal_marginal_relevance(sentence_vector, phrases, embedding_matrix, lambda_constant=0.5, threshold_terms=10):\n",
    "    \"\"\"\n",
    "    Return ranked phrases using MMR. Cosine similarity is used as similarity measure.\n",
    "    :param sentence_vector: Query vector\n",
    "    :param phrases: list of candidate phrases\n",
    "    :param embedding_matrix: matrix having index as phrases and values as vector\n",
    "    :param lambda_constant: 0.5 to balance diversity and accuracy. if lambda_constant is high ,      then higher accuracy. If lambda_constant is low then high diversity.\n",
    "    :param threshold_terms: number of terms to include in result set\n",
    "    :return: Ranked phrases with score\n",
    "    \"\"\"\n",
    "    # todo: Use cosine similarity matrix for lookup among phrases instead of making call everytime.\n",
    "    s = []\n",
    "    r = sorted(phrases, key=lambda x: x[1], reverse=True)\n",
    "    r = [i[0] for i in r]\n",
    "    while len(r) > 0:\n",
    "        score = 0\n",
    "        phrase_to_add = ''\n",
    "        for i in r:\n",
    "            first_part = cosine_similarity([sentence_vector], [embedding_matrix.loc[i]])[0][0]\n",
    "            second_part = 0\n",
    "            for j in s:\n",
    "                cos_sim = cosine_similarity([embedding_matrix.loc[i]], [embedding_matrix.loc[j[0]]])[0][0]\n",
    "                if cos_sim > second_part:\n",
    "                    second_part = cos_sim\n",
    "            equation_score = lambda_constant*(first_part-(1-lambda_constant) * second_part)\n",
    "            if equation_score > score:\n",
    "                score = first_part - (1 - lambda_constant) * second_part\n",
    "                phrase_to_add = i\n",
    "        if phrase_to_add == '':\n",
    "            phrase_to_add = i\n",
    "        r.remove(phrase_to_add)\n",
    "        s.append((phrase_to_add, score))\n",
    "    return (s, s[:threshold_terms])[threshold_terms > len(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePhrases(tokens,scores,filteredWordScores,cleaned_words,sort=False):\n",
    "    dictionary=set()\n",
    "    for word in cleaned_words:\n",
    "        dictionary.add(word)\n",
    "    phrases=dict()\n",
    "    bagOfWords=set()\n",
    "    for word in filteredWordScores:\n",
    "        bagOfWords.add(word[0])\n",
    "    phrase=\"\"\n",
    "    totalScore=0\n",
    "    wordCount=0\n",
    "    i=0\n",
    "    def reset():\n",
    "        nonlocal wordCount,phrase,totalScore\n",
    "        if(wordCount>0):\n",
    "            phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "            totalScore=0\n",
    "            wordCount=0\n",
    "            phrase=\"\"\n",
    "    while i in range(len(tokens)):\n",
    "        word=stemmer.lemmatize(tokens[i])\n",
    "        if (word in stops or tokens[i] in stops) and phrase!=\"\":\n",
    "            j=i+1\n",
    "            while j<len(tokens) and stemmer.lemmatize(tokens[j]) not in dictionary:\n",
    "                j+=1\n",
    "            if(j<len(tokens) and stemmer.lemmatize(tokens[j]) in bagOfWords):\n",
    "                for k in range(i,j):\n",
    "                    phrase+=tokens[k]+\" \"\n",
    "                i=j-1\n",
    "            else:\n",
    "                reset()\n",
    "        elif word in bagOfWords:\n",
    "            totalScore+=scores[word]\n",
    "            wordCount+=1\n",
    "            phrase+=tokens[i]+\" \"\n",
    "        else:\n",
    "            reset()\n",
    "        i+=1\n",
    "    phraseScores=list(map(list, phrases.items()))\n",
    "    if sort:\n",
    "        phraseScores=sorted(phraseScores, key = lambda x: x[1])#len(x[0].split()))#,reverse=True)\n",
    "    return phraseScores"
   ]
  }
 ]
}
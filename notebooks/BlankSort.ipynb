{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext as ft\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "if \"model\" not in globals():\n",
    "    model=ft.load_facebook_vectors(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3\n",
    "stemmer = nltk.wordnet.WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(text,top=0,collapse=False):\n",
    "    text=text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model.wv.vocab):\n",
    "            filtered_words.append(word)\n",
    "            counts.setdefault(word,0)\n",
    "            counts[word]+=1\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=model.wv.similarity(filtered_words[i],filtered_words[j])\n",
    "            similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        phraseScores=collapsePhrases(tokens,scores,filteredWordScores[:len(filteredWordScores)//3],cleaned_words)\n",
    "        if(top==0):\n",
    "            top=len(phraseScores)\n",
    "        return phraseScores[:top]\n",
    "    else:\n",
    "        if(top==0):\n",
    "            top=len(filteredWordScores)\n",
    "        return filteredWordScores[:top]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePhrases(tokens,scores,filteredWordScores,cleaned_words):\n",
    "    dictionary=set()\n",
    "    for word in cleaned_words:\n",
    "        dictionary.add(word)\n",
    "    phrases=dict()\n",
    "    bagOfWords=set()\n",
    "    usedBag=set()\n",
    "    for word in filteredWordScores:\n",
    "        bagOfWords.add(word[0])\n",
    "    phrase=\"\"\n",
    "    totalScore=0\n",
    "    wordCount=0\n",
    "    i=0\n",
    "    while i in range(len(tokens)):\n",
    "        word=stemmer.lemmatize(tokens[i])\n",
    "        if (word in stops or tokens[i] in stops) and phrase!=\"\":\n",
    "            j=i+1\n",
    "            while j<len(tokens) and stemmer.lemmatize(tokens[j]) not in dictionary:\n",
    "                j+=1\n",
    "            if(j<len(tokens) and stemmer.lemmatize(tokens[j]) in bagOfWords):\n",
    "                for k in range(i,j):\n",
    "                    phrase+=tokens[k]+\" \"\n",
    "                    usedBag.add(tokens[k])\n",
    "                i=j-1\n",
    "            else:\n",
    "                if(wordCount>0):\n",
    "                    if(wordCount>1 or phrase[:len(phrase)-1] not in usedBag):\n",
    "                        phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                    totalScore=0\n",
    "                    wordCount=0\n",
    "                    phrase=\"\"\n",
    "        elif word in bagOfWords:\n",
    "            totalScore+=scores[word]\n",
    "            wordCount+=1\n",
    "            usedBag.add(tokens[i])\n",
    "            phrase+=tokens[i]+\" \"\n",
    "        else:\n",
    "            if(wordCount>0):\n",
    "                if(wordCount>1 or phrase[:len(phrase)-1] not in usedBag):\n",
    "                    phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                totalScore=0\n",
    "                wordCount=0\n",
    "                phrase=\"\"\n",
    "        i+=1\n",
    "    phraseScores=list(map(list, phrases.items()))\n",
    "    phraseScores=sorted(phraseScores, key = lambda x: x[1])#,reverse=True)\n",
    "    return phraseScores"
   ]
  }
 ]
}
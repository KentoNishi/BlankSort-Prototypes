{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "import re\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\yoshi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": [
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "model=fasttext.load_model(os.path.join(os.getcwd(),\"binaries/data/cc.en.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3\n",
    "stemmer = nltk.wordnet.WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(w1,w2):\n",
    "    try:\n",
    "        return spatial.distance.cosine(model[w1],model[w2])\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(text,top=0,collapse=True):\n",
    "    text=text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.lemmatize(word) for word in tokens]\n",
    "    cleaned_words= [word for word in stemmed_words if word not in stops]\n",
    "    filtered_words=[]\n",
    "    counts=dict()\n",
    "    scores=dict()\n",
    "    for word in cleaned_words:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word)\n",
    "        if(len(filtered)>=3 or filtered in model):\n",
    "            filtered_words.append(word)\n",
    "            counts.setdefault(word,0)\n",
    "            counts[word]+=1\n",
    "    scoreList=np.zeros(len(filtered_words))\n",
    "    for i in range(len(filtered_words)):\n",
    "        leftBound=max(0,i-window_size)\n",
    "        rightBound=min(len(filtered_words)-1,i+window_size)\n",
    "        contextSize=rightBound-leftBound+1\n",
    "        for j in range(i+1,rightBound+1):\n",
    "            similarity_score=similarity(filtered_words[i],filtered_words[j])\n",
    "            similarity_score=(similarity_score+1)/2.0\n",
    "            scoreList[i]+=similarity_score\n",
    "            scoreList[j]+=similarity_score\n",
    "        wordScore=scoreList[i]/(counts[filtered_words[i]]*contextSize)\n",
    "        if filtered_words[i] not in scores:\n",
    "            scores[filtered_words[i]]=wordScore\n",
    "        else:\n",
    "            scores[filtered_words[i]]=min(scores[filtered_words[i]],wordScore)\n",
    "    wordScores=list(map(list, scores.items()))\n",
    "    wordScores=sorted(wordScores, key = lambda x: x[1])#,reverse=True)\n",
    "    filteredWordScores=[]\n",
    "    for word in wordScores:\n",
    "        filtered=re.sub('[^a-zA-Z]', '', word[0])\n",
    "        if(len(filtered)>2):\n",
    "            filteredWordScores.append([filtered,word[1]])\n",
    "    if collapse:\n",
    "        phraseScores=collapsePhrases(tokens,scores,filteredWordScores[:len(filteredWordScores)//3],cleaned_words)\n",
    "        if(top==0):\n",
    "            top=len(phraseScores)\n",
    "        return phraseScores[:top]\n",
    "    else:\n",
    "        if(top==0):\n",
    "            top=len(filteredWordScores)\n",
    "        return filteredWordScores[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePhrases(tokens,scores,filteredWordScores,cleaned_words):\n",
    "    dictionary=set()\n",
    "    for word in cleaned_words:\n",
    "        dictionary.add(word)\n",
    "    phrases=dict()\n",
    "    bagOfWords=set()\n",
    "    for word in filteredWordScores:\n",
    "        bagOfWords.add(word[0])\n",
    "    phrase=\"\"\n",
    "    totalScore=0\n",
    "    wordCount=0\n",
    "    i=0\n",
    "    while i in range(len(tokens)):\n",
    "        word=stemmer.lemmatize(tokens[i])\n",
    "        if (word in stops or tokens[i] in stops) and phrase!=\"\":\n",
    "            j=i+1\n",
    "            while j<len(tokens) and stemmer.lemmatize(tokens[j]) not in dictionary:\n",
    "                j+=1\n",
    "            if(j<len(tokens) and stemmer.lemmatize(tokens[j]) in bagOfWords):\n",
    "                for k in range(i,j):\n",
    "                    phrase+=tokens[k]+\" \"\n",
    "                i=j-1\n",
    "            else:\n",
    "                if(wordCount>0):\n",
    "                    phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                    totalScore=0\n",
    "                    wordCount=0\n",
    "                    phrase=\"\"\n",
    "        elif word in bagOfWords:\n",
    "            totalScore+=scores[word]\n",
    "            wordCount+=1\n",
    "            phrase+=tokens[i]+\" \"\n",
    "        else:\n",
    "            if(wordCount>0):\n",
    "                phrases[phrase[:len(phrase)-1]]=totalScore/wordCount\n",
    "                totalScore=0\n",
    "                wordCount=0\n",
    "                phrase=\"\"\n",
    "        i+=1\n",
    "    phraseScores=list(map(list, phrases.items()))\n",
    "    phraseScores=sorted(phraseScores, key = lambda x: x[1])#,reverse=True)\n",
    "    return phraseScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[['learning', 0.233530370252473],\n ['word', 0.25416457106669743],\n ['fasttext', 0.3708248118782649],\n ['model', 0.3915905353746244],\n ['facebook', 0.4057554890002523],\n ['embedding', 0.6735601257532835],\n ['unsupervised', 0.7427900529333523],\n ['supervised', 0.7448886116700513],\n ['library', 0.7532016724348068],\n ['embeddings', 0.7625067649143082],\n ['algorithm', 0.7657674902251789],\n ['allows', 0.7666801608034542],\n ['create', 0.7698574385472706],\n ['representation', 0.771623567977388],\n ['text', 0.7731904089450836],\n ['network', 0.7750661433674395],\n ['vector', 0.7764924115368298],\n ['lab', 0.7854030651173421],\n ['obtaining', 0.7872626007135425],\n ['neural', 0.7882439150874104],\n ['research', 0.7890130979108757],\n ['make', 0.7949823731822627],\n ['language', 0.7999857480504683],\n ['pretrained', 0.8012162328564695],\n ['classification', 0.801532802038959],\n ['available', 0.8017196925356984],\n ['created', 0.8121793105133942]]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(\"\"\"\n",
    "fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. fastText uses a neural network for word embedding.\n",
    "\"\"\",0,False)"
   ]
  }
 ]
}